---
title: "Gaussian process emulation"
author: "Ciarán McInerney"
date: "2023-06-26"
output:
  html_document:
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Requisites
```{r Requisites, warning = FALSE, message = FALSE}
# Load packages.
library(tidyverse)
library(RobustGaSP)
library(FME)
library(ggdist)
library(gridExtra)
# Instantiate shared variables.
sd <- 2
n <- 1000
# Set seed.
set.seed(1)
# Set reusable plot formatting.
plotformatting <-
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.text.y = element_blank(),
        strip.text = element_blank()
        )
```
## TL;DR
**...**
<br/><br/>
<br/><br/>

## Why?

The origin of emulation is often said to arise from the work of Jerome Sacks and friends in their "[Design and Analysis of Computer Experiments](https://sci-hub.wf/https://www.jstor.org/stable/2245858)". The Bayesian approach was then developed and promoted by Carla Currin and her pals. An academic summary can be found in [Currin et al. (1991)](https://sci-hub.wf/https://www.jstor.org/stable/2290511) which builds a bit on their previous work in [Currin et al. (1989)](https://technicalreports.ornl.gov/cpr/rpt/6863.pdf).
<br/><br/>
<br/><br/>

## Seriously, what?
I like [Andrianakis et al.](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1003968&type=printable)'s description of emulators as Bayesian representations of a simulator. It communicates two key notions:

1. that the emulator isn't trying to model the simulator with fidelity - and definitely not model reality with fidelity, and

2. that the emulator is based in abstract probabilistic relations between the inputs and outputs.

...
A Gaussian process emulator is trying to find the functions that fit
...

Emulators are not restricted to Gaussian processes. For example, [Hooten et al.](https://www.stat.colostate.edu/~hooten/papers/pdf/Hooten_etal_JABES_2011.pdf) use random forests as their emulator.


### ...and what are covariance functions?
<br/><br/>
<br/><br/>


## How?
To illustrate a Gaussian process emulator, I'll use the two-input toy example from [Bastos and O'Hagan](https://sci-hub.wf/10.1198/tech.2009.08019).
<br/><br/>
<br/><br/>
Each of $n$ iterations, $i=(1, 2, \ldots , n)$, of the simulation takes two real-numbered inputs, $a$ and $b$, with range between 0 and 1 inclusive, to produce an output, $y$, according to the function
$$
y = (1-e^{-\frac{1}{2b}}) \left(\frac{2300a^3+1900a^2+2092a+60}{100a^3+500a^2+4a+20}\right)
$$
I'll fit a Gaussian process emulator on the inputs and outputs of 20 simulations; these 20 tuples will provide the 'train' dataset. I'll assess my Gaussian process emulator on a separate set of inputs and outputs from another 25 simulations; these 25 tuples will provide the 'test' dataset. The values for inputs will be selected by [latin hypercube sampling]().
<br/><br/>
<br/><br/>
Let's crunch those numbers and view the first few tuples of $(a, b, y)$.
```{r Simulation, }
# Set simulation parameters.
n_train <- 20
n_test <- 25
# Set input values.
possVals <- data.frame(min = c(0, 0), max = c(1, 1))
rownames(possVals) <- c("a", "b")
df_GPE_train <- FME::Latinhyper(possVals, n_train) %>% data.frame()
df_GPE_test <- FME::Latinhyper(possVals, n_test) %>% data.frame()
# Calculate and append output values.
df_GPE_train <-
  df_GPE_train %>%
  dplyr::mutate(
    y = 
      (
      1 - exp(-(1 / (2*b)) )
    ) *
      (
        sum( (2300*a^3), (1900*a^2), (2092*a), 60 ) /
          sum( (100*a^3), (500*a^2), (4*a), 20 )
      )
  )
df_GPE_test <-
  df_GPE_test %>%
  dplyr::mutate(
    y = 
      (
      1 - exp(-(1 / (2*b)) )
    ) *
      (
        sum( (2300*a^3), (1900*a^2), (2092*a), 60 ) /
          sum( (100*a^3), (500*a^2), (4*a), 20 )
      )
  )
# Display first and last few tuples of training dataset.
knitr::kable(head(df_GPE_train), format = "html")
```
<br/><br/>
<br/><br/>
I will use a Gaussian covariance / correlation function (a.k.a. a kernel) to define how any two given values of an input - i.e. either $a$ and $a^{\prime}$ or $b$ and $b^{\prime}$ - co-vary. If we only had a one-dimensional input, $a$, then the Gaussian covariance function would simply be
$$
C(a, a^{\prime},\psi_{a})=exp \left( - \left( \frac{a-a^{\prime}} {\psi_{a}} \right)^2  \right)
$$
where $\psi_{a}$ is the correlation length parameter (a.k.a. the characteristic length-scale of the process) for input $a$, which is estimated when fitting the emulator. As the denominator of the fraction, it scales / tempers the difference between two given values of $a$, effectively dictating the reach of the correlation influence between values of $a$.
<br/>
A larger length parameter leads to a smaller quotient being squared, which leads to a smaller correlation, for a given difference. As you might already have inferred, the choice of the covariance function and its parameters affect how smooth our emulator outputs are. For example, functions are smoother if the correlation is stronger over larger differences in the values.
<br/><br/>
<br/><br/>
Of course, we have a two-dimensional input denoted by the tuple $(a_{k}, b_{k})$ so the covariance function needs to accommodate both dimensions. Following the particular multi-dimensional Gaussian covariance function used by [Bastos and O'Hagan](https://sci-hub.wf/10.1198/tech.2009.08019), we get
$$
C((a, b), (a^{\prime}, b^{\prime}), (\psi_{a}, \psi_{b})) =exp \left( - \left( \frac{a-a^{\prime}} {\psi_{a}} \right)^2  + \left( \frac{b-b^{\prime}} {\psi_{b}} \right)^2\right)
$$
```{r Fit the GPE}
(model <-
  RobustGaSP::rgasp(design = df_GPE_train %>% dplyr::select(a, b) %>% as.matrix(),
                    response = df_GPE_train %>% dplyr::select(y)%>% as.matrix(),
                    #kernel_type = 'pow_exp',
                    #alpha = 2,
                    lower_bound = FALSE)
)
```

...
<br/><br/>
<br/><br/>

### But is it any good?
Just because you can, doesn't mean you should. If we use Gaussian process emulators, then we should evaluate how well they are emulating the simluation of the real world, rather than just assuming that they are representing the real-world well. Never forget that emulators are models of models - that's twice removed from the reality about which we want to make inference.
<br/><br/>
A worthwhile read on this topic is [Leonardo Bastos](https://scholar.google.com/citations?user=A5VmZYMAAAAJ&hl=en&oi=sra) and [Anthony O’Hagan](https://scholar.google.com/citations?user=CljxR1UAAAAJ&hl=en&oi=sra)'s 2009 paper on "[Diagnostics for Gaussian process emulators](https://sci-hub.wf/10.1198/tech.2009.08019)", which they wrote during their time at University of Sheffield. The fundamental idea is that models' validity is usually assessed by comparing observations to predictions, but these are one and the same for emulators. The trick is to notice that the observations from the simulator and the predictions from the emulator are only the same for the observations used to 'train' / fit the emulator. A 'test' set of observations can be provided by generating new observations from the simulator that the emulator has never seen. Then, all that is needed is a suitable distance measure and maybe a decision rule to indicate if the emulator is sufficient.
<br/><br/>
The importance of a "suitable" distance measure cannot be underestimated. If our simulator or our emulator output is a smooth function, then there is dependence between successive values. Any distance measure that assumes _independence_ between observations would not be suitable, rendering the assessment of emulator validity invalid! I particularly like [Bastos and O'Hagan](https://sci-hub.wf/10.1198/tech.2009.08019)'s choice of pivoted Cholesky decomposition of the Mahalanobis distance between simulator and emulator outputs for a given set of 'test' inputs unseen by the emulator during 'training' / fitting...

```{r B&B what, fig.align = "center", out.width = "50%"}
knitr::include_graphics("https://media.giphy.com/media/3oKHWa8DyEfPc3baCc/giphy.gif")
```
<br/><br/>
<br/><br/>

Ok, that was a bit of a mouthful. To be fair, the concept is a bit of a headful but let me try to give you the gist, in a few bullet points:

- The Mahalanobis distance, $D_{Mahalanobis}$ , is a way of measuring the distance from a point to a distribution of points. This makes it suitable to our situation of comparing a single outcome observation to the posterior distribution of outcomes provided by our Gaussian process emulator.

- The Mahalanobis distance makes use of [principal components](https://en.wikipedia.org/wiki/Principal_component), which enables it to handle dependence between data points. This makes it suitable to our situation where the simulation or emulator are smooth functions.

- The Mahalanobis distance can be decomposed into the dot product of a vector of transformed errors, $\textbf{A}$, and its transpose, $\textbf{A}^T$: $D_{Mahalanobis} = A^TA$. The "error" in this case is the difference between the simulated and the emulated outputs using the unseen 'test' inputs. The errors can be transformed in variety of ways by some matrix $\textbf{B}$ to ensure, for example, uncorrelated elements with unit variances. All that matters is that $\textbf{A}=\textbf{B}(output_{simulation,\ 'test'}-output_{emulation,\ 'test'})$.

- A [Cholesky deomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of the variance of emulations gives us a computationally-efficient transformation matrix to use. Better still, a _pivoted_ Choleskey decomposition sorts the pivots such that the first refers to emulated values with the largest variance, the second refers to emulated values with the second-largest, etc.

- Bastos and O'Hagan argue that the first few pivots relate to (non-)homogeneity of emulations for a given input or non-stationarity, and the latter pivots relate to the appropriateness of the correlation structure we specified for our emulator.

- This all makes for a decomposition and distance measure that enable us to distinguish the source of any poor emulation.

Bastos and O'Hagan aren't the first to know about decomposing Mahalanobis distances; they just leveraged insight from [Myung Geum Kim](https://sci-hub.wf/10.1080%2F03610920008832559) some nine years earlier.
<br/><br/>
What are some of the other ways to check the emulation?

- A histogram and summary statistics of the Mahalanobis distances gives an idea of the distribution of emulator performance.

```{r Mahalanobis diagnostics, fig.align = "center", out.width = "50%"}
# Calculate the Mahalanobis distances for s-many simulations using a test dataset.
emul_errors <- 
  outputs_sim_testset - # A 1-by-s vector of simulation outputs.
    mean(outputs_emul_testset) # A 1-by-s vector of means of emulation outputs per simulation.
mahalanobis_distances <-
  emul_errors %*% # A 1-by-s vector of errors.
  solve(A) %*% # The inverse of an s-by-s covariance matrix of the emulations. The `solve()` command does the inverting.
  t(emul_errors) # An s-by-1 vector of errors.
# Plot the distribution of Mahalanobis distances.
(p_MahalanobisDist <-
    mahalanobis_distances %>%
    ggplot(aes(x = ???)) +
    geom_histogram(aes(y = ..density..),
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666")
)
# Output the summary statistics of the Mahalnobis distances.
print(
  summary(mahalanobis_distances)
  )
...
```

- Plotting emulator errors against the index of pivots from the pivoted Cholesky decomposition should show a spread around 0 and constant variance, testing homogeneity, stationarity, and approrpriateness of the correlation structure.

```{r Error-against-index plot, fig.align = "center", out.width = "50%"}
# Assumes `uncorr_standardised_errors` is the matrix of uncorrelated standardised
# errors. Try to provide the pivoted Cholesky decomposition matrix.
#
# Also, make sure to apprend a column called `index` that gives the row number.
(p_errorAgainstIndex <-
   df_errorsAndIndex %>%
   ggplot(aes(x = index, y = uncorr_standardised_errors)) +
    geom_point()
)
```

- A run-of-the-mill Q-Q plot of the uncorrelated standardised errors tests our assumption that simulator outputs were Gaussian distributed.

```{r Q-Q plot, fig.align = "center", out.width = "50%"}
# Assumes `uncorr_standardised_errors` is the matrix of uncorrelated standardised
# errors. Try to provide the pivoted Cholesky decomposition matrix.
(p_QQplot <-
   df_errorsAndIndex %>%
   ggplot(aes(sample = uncorr_standardised_errors)) +
    stat_qq() +
    stat_qq_line()
)
```

- Plotting the distribution of emulator errors against their respective input values tests for patterns of emulation errors for particular portions of the input space.

```{r Error-against-input plot, fig.align = "center", out.width = "50%" }
(p_errorAgainstInput <-
   df_errorsAndIndex %>%
   ggplot(aes(x = input_val, y = uncorr_standardised_errors)) +
    geom_point(
      shape = 95,
      size = 10,
      alpha = 0.2)
  
)
```


## Final thoughts and other resources
...good because...
<br/><br/>

### It ain't magic
[As I noted in my explainer of Gaussian processes](), they are not some sort of magic that frees the user of difficult decisions just because it replaces a resource-consuming simulation. These "difficult decisions" I'm talking about are the assumptions that a method relies on. One assumption of a Gaussian process emulator is that each point in the simulation being emulated is Gaussian distributed and [stationary](https://en.wikipedia.org/wiki/Stationary_process) over the duration being simulated. This is only justified if the underlying data-generating phenomenon is in a consistent / stable state rather than switching between different states, or worse still, ever-changing. The variance we observe must be due to randomness, only.
<br/><br/>
In the case of climate modelling, it is arguable that the Gaussian process emulation's assumption of stationarity will only be valid for emulating simulations based on stable physical laws, rather than, say, some well-fitted-but-merely-a-proxy simulation. To ponder more about this issue of proxy models, I highly recommend the fourth chapter of Richard McElreath's book "[Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)" or watching [its accompanying lecture recording](https://youtu.be/tNOu-SEacNU)
<br/><br/>

### Is it normal?
I'm glad [Bastos and O'Hagan](https://sci-hub.wf/10.1198/tech.2009.08019) note that

> "the emulator asserts that it is very unlikely for the true simulator output to be more than two or three predictive standard deviations from the predictive mean, and that it is no more likely to be above the predictive mean than below it"

In a world of [complex adaptive systems](https://youtu.be/GjwvsK-6640), [black swans](https://en.wikipedia.org/wiki/Black_swan_theory), [heavy- and long- tailed distributions](https://youtu.be/vIp1kY0H0yw), and [scale-free distributions](https://youtu.be/pYMsI-8GsxI), are Gaussian assumptions appropriate?
The Gaussian assumptions can be checked using, for example, Q-Q plots of the uncorrelated standardised errors and interpretting as usual (i.e. points on the lines means good).
<br/><br/>
<br/><br/>

### Resources

1.	For a a good introduction to Gaussian process emulation, check our University of Bristol's offer (https://compass.blogs.bristol.ac.uk/2022/01/25/gaussian-process-emulation/).

2.	[distill.pub](https://distill.pub) publications also have a blog by [Agnihotri and Batra](https://distill.pub/2020/bayesian-optimization/) on Bayesian optimization that references Krige’s initial work.

3. Section 2.2 of [Bastos and O'Hagan](https://sci-hub.wf/10.1198/tech.2009.08019) covers other problems with Gaussian process emulators.

